{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ef0b205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dependencies\n",
    "\n",
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "\n",
    "import os.path\n",
    "import xml.etree.ElementTree as ET                  # necessary to process .xml files\n",
    "\n",
    "from random import shuffle\n",
    "from keras.preprocessing import sequence            # necessary for padding\n",
    "from keras.models import Sequential                 # Base Keras NN model\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D # Convolution layer and pooling\n",
    "from keras.layers import Dense, Dropout, Activation # The objects for each layer\n",
    "from nltk.tokenize import TreebankWordTokenizer     # Tokenizer\n",
    "from nltk.stem.porter import PorterStemmer          # Stemmer\n",
    "from gensim.models.keyedvectors import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "853b0392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load this data\n",
    "\n",
    "def pre_process_data(filepath):\n",
    "    male_path = os.path.join(filepath, 'male')\n",
    "    female_path = os.path.join(filepath, 'female')\n",
    "    male_label = 1\n",
    "    female_label = 0\n",
    "    dataset = []\n",
    "    for filename in glob.glob(os.path.join(male_path, '*.txt')):\n",
    "        with open(filename, 'r', encoding=\"utf-8\") as f:\n",
    "            dataset.append((male_label, f.read()))\n",
    "    for filename in glob.glob(os.path.join(female_path, '*.txt')):\n",
    "        with open(filename, 'r', encoding=\"utf-8\") as f:\n",
    "            dataset.append((female_label, f.read()))\n",
    "    shuffle(dataset)\n",
    "    return dataset\n",
    "\n",
    "def collect_expected(dataset):\n",
    "    return [sample[0] for sample in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35aee76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_path = \"C:\\\\Users\\\\morzm\\\\jup_txts\\\\merged_corpus\\\\\"\n",
    "#google_vectors = \"C:\\\\Users\\\\morzm\\\\jup_txts\\\\GoogleNews-vectors-negative300.bin.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d78a9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the data\n",
    "dataset = pre_process_data(corpus_path)\n",
    "#dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "316cfce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the fasttext embeddings\n",
    "\n",
    "fasttext_vectors = 'crawl-300d-2M.vec'\n",
    "word_vectors = KeyedVectors.load_word2vec_format(fasttext_vectors, binary=False, limit=400000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee1f0aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO STEMMING AND STOPWORDS\n",
    "\n",
    "my_word_stop = ['the', 'in', 'of', 'is', 'a', 'to', 'an', 'be']\n",
    "\n",
    "def tokenize_and_vectorize(dataset):\n",
    "    tokenizer = TreebankWordTokenizer()\n",
    "    vectorized_data = []\n",
    "\n",
    "    for sample in dataset:\n",
    "        lowercase = sample[1].lower()\n",
    "        tokens = tokenizer.tokenize(lowercase)\n",
    "        sample_vecs = []\n",
    "        for token in tokens:\n",
    "            if token not in my_word_stop:\n",
    "                #print(token)\n",
    "                try:\n",
    "                    sample_vecs.append(word_vectors[token])\n",
    "                except KeyError:\n",
    "                    pass # No matching token in the Google w2v vocab\n",
    "        vectorized_data.append(sample_vecs)\n",
    "\n",
    "    return vectorized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec2cedf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to get the target labels\n",
    "def collect_expected(dataset):\n",
    "    \"\"\" Peel off the target values from the dataset \"\"\"\n",
    "    expected = []\n",
    "    for sample in dataset:\n",
    "        expected.append(sample[0])\n",
    "    return expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6cced169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "\n",
    "vectorized_data = tokenize_and_vectorize(dataset)\n",
    "expected = collect_expected(dataset)\n",
    "\n",
    "#vectorized_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "8797f033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3001\n",
      "2100\n",
      "2550\n"
     ]
    }
   ],
   "source": [
    "# Creating partitions\n",
    "# 70% train | 15% validate | 15% test\n",
    "\n",
    "split_point = int(len(vectorized_data)*.7)\n",
    "further_split = int(split_point + len(vectorized_data)*.15)\n",
    "print(len(vectorized_data))\n",
    "print(split_point)\n",
    "print(further_split)\n",
    "x_train = vectorized_data[:split_point]\n",
    "y_train = expected[:split_point]\n",
    "x_val = vectorized_data[split_point:further_split]\n",
    "y_val = expected[split_point:further_split]\n",
    "x_test = vectorized_data[further_split:]\n",
    "y_test = expected[further_split:]\n",
    "# should work. go from 0 to a then from a to b then from b to end\n",
    "\n",
    "# FOR REFERENCE\n",
    "# split_point = int(len(vectorized_data)*.7)\n",
    "# x_train = vectorized_data[:split_point] # there's a typo in this line, if copying from the book\n",
    "# y_train = expected[:split_point]\n",
    "# x_test = vectorized_data[split_point:]\n",
    "# y_test = expected[split_point:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "da74cd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network parameters\n",
    "\n",
    "maxlen = 600          # 600\n",
    "batch_size = 32       # no difference on accuracy\n",
    "embedding_dims = 300  # Same as Google's\n",
    "filters = 250\n",
    "kernel_size = 3       # 3 or 5, no difference on accuracy\n",
    "hidden_dims = 250\n",
    "epochs = 4            # 2 or 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "b35ed1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to pad or truncate the input\n",
    "# (notice that this code is quite verbose)\n",
    "def pad_trunc(data, maxlen):\n",
    "    \"\"\"\n",
    "    For a given dataset pad with zero vectors or truncate to maxlen\n",
    "    \"\"\"\n",
    "    new_data = []\n",
    "    # Create a vector of 0s the length of our word vectors\n",
    "    zero_vector = []\n",
    "    for _ in range(len(data[0][0])):\n",
    "        zero_vector.append(0.0)\n",
    "\n",
    "    for sample in data:\n",
    "        if len(sample) > maxlen:\n",
    "            temp = sample[:maxlen]\n",
    "        elif len(sample) < maxlen:\n",
    "            temp = sample\n",
    "            # Append the appropriate number 0 vectors to the list\n",
    "            additional_elems = maxlen - len(sample)\n",
    "            for _ in range(additional_elems):\n",
    "                temp.append(zero_vector)\n",
    "        else:\n",
    "            temp = sample\n",
    "        new_data.append(temp)\n",
    "\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "16c37297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding/truncating the data (if necessary)\n",
    "\n",
    "x_train = pad_trunc(x_train, maxlen)\n",
    "x_val = pad_trunc(x_val, maxlen)\n",
    "x_test = pad_trunc(x_test, maxlen)\n",
    "\n",
    "# The shape is [number of samples, sequence length, word vector]  CUBE \n",
    "x_train = np.reshape(x_train, (len(x_train), maxlen, embedding_dims))\n",
    "y_train = np.array(y_train)\n",
    "x_val = np.reshape(x_val, (len(x_val), maxlen, embedding_dims))\n",
    "y_val = np.array(y_val)\n",
    "x_test = np.reshape(x_test, (len(x_test), maxlen, embedding_dims))\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "5b27b56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n"
     ]
    }
   ],
   "source": [
    "print('Building model...')\n",
    "model = Sequential()   # The standard NN model\n",
    "model.add(Conv1D(      # Adding a convolutional layer\n",
    "    filters,\n",
    "    kernel_size,\n",
    "    padding='valid',   # in this example the output is going to be lightly smaller\n",
    "    activation='relu',\n",
    "    strides=1,         # the shift\n",
    "    input_shape=(maxlen, embedding_dims))\n",
    "    )\n",
    "\n",
    "#model.summary()\n",
    "# Formulation: max (0, dot(filter, 3-gram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "1b1a6fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the max pooling. What is max pooling? For each filter one new version of the instance is produced, Pooling evenly divides the output of each filter into subsections\n",
    "#It selects (or computes) a representative value for each subsection.\n",
    "# Alternatives \n",
    "# - GlobalMaxPooling1D() (the max for the entire filter's output)\n",
    "# - MaxPooling1D(n)  (the max for a specific area of n; default n=2)\n",
    "# - AvgPooling1D(n)\n",
    "\n",
    "model.add(GlobalMaxPooling1D())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "114197ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding dropout (20% of the data will be \"cancelled\")\n",
    "model.add(Dense(hidden_dims))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Activation('relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "eef892bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The classification layer!\n",
    "# sigmoid range: [0,1]\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "228a4c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_18 (Conv1D)          (None, 598, 250)          225250    \n",
      "                                                                 \n",
      " global_max_pooling1d_18 (Gl  (None, 250)              0         \n",
      " obalMaxPooling1D)                                               \n",
      "                                                                 \n",
      " dense_36 (Dense)            (None, 250)               62750     \n",
      "                                                                 \n",
      " dropout_18 (Dropout)        (None, 250)               0         \n",
      "                                                                 \n",
      " activation_36 (Activation)  (None, 250)               0         \n",
      "                                                                 \n",
      " dense_37 (Dense)            (None, 1)                 251       \n",
      "                                                                 \n",
      " activation_37 (Activation)  (None, 1)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 288,251\n",
      "Trainable params: 288,251\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Compiling the CNN\n",
    "model.compile(loss='binary_crossentropy',\n",
    "        optimizer='adam',   # don't question it, just use adam\n",
    "        metrics=['accuracy']\n",
    "        )\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "b455fd3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "66/66 [==============================] - 16s 121ms/step - loss: 0.7046 - accuracy: 0.5571 - val_loss: 0.6320 - val_accuracy: 0.6400\n",
      "Epoch 2/4\n",
      "66/66 [==============================] - 7s 106ms/step - loss: 0.5809 - accuracy: 0.7010 - val_loss: 0.6273 - val_accuracy: 0.6444\n",
      "Epoch 3/4\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 0.4513 - accuracy: 0.7933 - val_loss: 0.5677 - val_accuracy: 0.6756\n",
      "Epoch 4/4\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.2707 - accuracy: 0.9114 - val_loss: 0.5410 - val_accuracy: 0.7356\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2588597bd60>"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting (training) the model\n",
    "model.fit(x_train, y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=(x_val, y_val)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "f673326a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on test data\n",
      "4/4 [==============================] - 1s 136ms/step - loss: 0.4513 - accuracy: 0.8182\n",
      "test loss, test acc: [0.45132648944854736, 0.8181818127632141]\n",
      "Generate predictions for 3 samples\n",
      "predictions shape: (3, 1)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test data using `evaluate`\n",
    "print(\"Evaluate on test data\")\n",
    "results = model.evaluate(x_test, y_test, batch_size=128)\n",
    "print(\"test loss, test acc:\", results)\n",
    "\n",
    "# Generate predictions (probabilities -- the output of the last layer)\n",
    "# on new data using `predict`\n",
    "print(\"Generate predictions for 3 samples\")\n",
    "predictions = model.predict(x_test[:3])\n",
    "print(\"predictions shape:\", predictions.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "c4505dfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[164,  52],\n",
       "       [ 29, 205]], dtype=int64)"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "result = confusion_matrix = confusion_matrix(y_test, np.rint(y_pred))\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "99c01c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "model_structure = model.to_json()\n",
    "with open(\"fasttext_cnn_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_structure)  # saves just the architecture\n",
    "model.save_weights(\"fasttext_cnn_weights.h5\")  # saves the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "02d0a893",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5532177]], dtype=float32)"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicting a new instance\n",
    "\n",
    "sample_1 = \"C:\\\\Users\\\\morzm\\\\jup_txts\\\\corpus\\\\male\\\\aa4b605f6679148ff186c46a616bfe8a.txt\"\n",
    "\n",
    "vec_list = tokenize_and_vectorize([(1, sample_1)])\n",
    "test_vec_list = pad_trunc(vec_list, maxlen)\n",
    "test_vec = np.reshape(test_vec_list, (len(test_vec_list), maxlen,\\\n",
    "        embedding_dims))\n",
    "model.predict(test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "aefa7301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      "Male\n"
     ]
    }
   ],
   "source": [
    "# Get the class\n",
    "print((model.predict(test_vec) > 0.5).astype(\"int32\"))\n",
    "\n",
    "if model.predict(test_vec) > 0.5:\n",
    "    print(\"Male\")\n",
    "else:\n",
    "    print(\"Female\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006e08bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
