{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9942e289",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import os.path\n",
    "import xml.etree.ElementTree as ET\n",
    "import csv\n",
    "import getopt\n",
    "import sys\n",
    "import json\n",
    "import copy\n",
    "from random import shuffle\n",
    "from sklearn import utils\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import keras\n",
    "import keras.preprocessing.text as kpt\n",
    "import nltk\n",
    "from keras.models import Sequential\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Dense, Dropout, Activation, LSTM, Input, Embedding\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Input, Dense, concatenate, Activation, Average\n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "from keras.callbacks import CSVLogger\n",
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff8bcb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = ET.parse(\"C:\\\\Users\\\\morzm\\\\jup_txts\\\\test\\\\test1.xml\")\n",
    "root = tree.getroot()\n",
    "n = 0\n",
    "\n",
    "save_path = \"C:\\\\Users\\\\morzm\\\\jup_txts\\\\corpus\\\\test\\\\\"\n",
    "\n",
    "for tweets in root.find('documents'):\n",
    "    tweet = tweets.text\n",
    "    text = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', tweet)\n",
    "    line_to_write = text + '\\n'\n",
    "    n += 1\n",
    "    with open(os.path.join(save_path,\"output\"+str(n)+\".txt\"), 'w', encoding=\"utf-8\") as f:\n",
    "        f.write(line_to_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de97b5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "xmls_directory = \"C:\\\\Users\\\\morzm\\\\jup_txts\\\\text\\\\\" # This is the path to the .xml files of the PAN2018 twitter corpus\n",
    "\n",
    "truth_path = \"C:\\\\Users\\\\morzm\\\\jup_txts\\\\en.txt\" # This is the path to the .txt file containing the ids and genders of each twitter user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "100e9df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This bit of code transforms the .xml files containing the tweets\n",
    "# on the base of a truth file into .txts, depending on gender\n",
    "# thus creating a corpus\n",
    "\n",
    "with open(truth_path, 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        string = line.partition(\":::\") # This line divides the author_id from the gender of the author\n",
    "        if \"male\\n\" in string: # this will ignore the last line of the list, but was the best solution I could come up with\n",
    "            save_path = \"C:\\\\Users\\\\morzm\\\\jup_txts\\\\corpus\\\\male\\\\\"\n",
    "            author_id = string[0]\n",
    "            for file in os.listdir(xmls_directory):\n",
    "                if file.endswith(author_id+\".xml\"):\n",
    "                    os.chdir(\"C:\\\\Users\\\\morzm\\\\jup_txts\\\\text\\\\\")\n",
    "                    xml_file = file # so that the file can be parsed by ElementTree\n",
    "                    tree = ET.parse(xml_file)\n",
    "                    root = tree.getroot()\n",
    "                    n = 0\n",
    "                    \n",
    "                    for tweets in root.find('documents'):\n",
    "                        tweet = tweets.text\n",
    "                        text = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', tweet)\n",
    "                        line_to_write = text + '\\n'\n",
    "                        n += 1\n",
    "                        with open(os.path.join(save_path, author_id+str(n)+\".txt\"), 'a', encoding=\"utf-8\") as f:\n",
    "                            f.write(line_to_write)\n",
    "                              \n",
    "        elif \"female\\n\" in string: # same for female authors\n",
    "            save_path = \"C:\\\\Users\\\\morzm\\\\jup_txts\\\\corpus\\\\female\\\\\"\n",
    "            author_id = string[0]\n",
    "            for file in os.listdir(xmls_directory):\n",
    "                if file.endswith(author_id+\".xml\"):\n",
    "                    os.chdir(\"C:\\\\Users\\\\morzm\\\\jup_txts\\\\text\\\\\")\n",
    "                    xml_file = file\n",
    "                    tree = ET.parse(xml_file)\n",
    "                    root = tree.getroot()\n",
    "                    n = 0\n",
    "                    \n",
    "                    for tweets in root.find('documents'):\n",
    "                        tweet = tweets.text\n",
    "                        text = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', tweet)\n",
    "                        line_to_write = text + '\\n'\n",
    "                        n += 1\n",
    "                        with open(os.path.join(save_path, author_id+str(n)+\".txt\"), 'a', encoding=\"utf-8\") as f:\n",
    "                            f.write(line_to_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e95b6e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_data(filepath):\n",
    "    male_path = os.path.join(filepath, 'male')\n",
    "    female_path = os.path.join(filepath, 'female')\n",
    "    male_label = 1\n",
    "    female_label = 0\n",
    "    dataset = []\n",
    "    for filename in glob.glob(os.path.join(male_path, '*.txt')):\n",
    "        with open(filename, 'r', encoding=\"utf-8\") as f:\n",
    "            dataset.append((male_label, f.read()))\n",
    "    for filename in glob.glob(os.path.join(female_path, '*.txt')):\n",
    "        with open(filename, 'r', encoding=\"utf-8\") as f:\n",
    "            dataset.append((female_label, f.read()))\n",
    "    shuffle(dataset)\n",
    "    return dataset\n",
    "\n",
    "def collect_expected(dataset):\n",
    "    return [sample[0] for sample in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d6646a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(word):\n",
    "    regexp = r'^(.*?)(ing|ious|ment)?$'\n",
    "    stem, suffix = re.findall(regexp, word)[0]\n",
    "    return stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f3a21214",
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAGS = re.MULTILINE | re.DOTALL\n",
    "\n",
    "def tokenize(text):\n",
    "    # Different regex parts for smiley faces\n",
    "    eyes = r\"[8:=;]\"\n",
    "    nose = r\"['`\\-]?\"\n",
    "\n",
    "    # function so code less repetitive\n",
    "    def re_sub(pattern, repl):\n",
    "        return re.sub(pattern, repl, text, flags=FLAGS)\n",
    "\n",
    "    # text = re_sub(r\"#\\S+\", \"hashtag\")\n",
    "    # text = re_sub(r\"([!?.]){2,}\", r\" \\1 \")\n",
    "    # text = re_sub(r\"\\b(\\S*?)(.)\\2{2,}\\b\", r\"\\1\\2 \")\n",
    "    # text = re_sub(r\"([A-Z]){2,}\", \"allcaps\")\n",
    "    # text = re_sub(r'([\\w!.,?();*\\[\\]\":\\‚Äù\\‚Äú])([!.,?();*\\[\\]\":\\‚Äù\\‚Äú])', r'\\1 \\2')\n",
    "    # text = re_sub(r'([!.,?();*:\\[\\]\":\\‚Äù\\‚Äú])([\\w!.,?();*\\[\\]\":\\‚Äù\\‚Äú])', r'\\1 \\2')\n",
    "    # text = re_sub(r'(.)(<)', r'\\1 \\2')\n",
    "    # text = re_sub(r'(>)(.)', r'\\1 \\2')\n",
    "    # text = re_sub(r'[\\'\\`\\‚Äô\\‚Äò]', r'')\n",
    "    # text = re_sub(r'\\\\n', r' ')\n",
    "    # text = re_sub(r\"https?:\\/\\/\\S+\\b|www\\.(\\w+\\.)+\\S*\", \" \")\n",
    "    # text = re_sub(r\"#(\\S+)\", r\" \") # replace #name with name\n",
    "    text = re_sub(r\"(:\\s?\\)|:-\\)|\\(\\s?:|\\(-:|:\\'\\))\", \" em_positive \")  # Smile -- :), : ), :-), (:, ( :, (-:, :')\n",
    "    text = re_sub(r\"(:\\s?D|:-D|x-?D|X-?D)\", \" em_positive \")  # Laugh -- :D, : D, :-D, xD, x-D, XD, X-D\n",
    "    text = re_sub(r\"(<3|:\\*)\", \" em_positive \")  # Love -- <3, :*\n",
    "    text = re_sub(r\"(;-?\\)|;-?D|\\(-?;)\", \" em_positive \")  # Wink -- ;-), ;), ;-D, ;D, (;,  (-;\n",
    "    text = re_sub(r'(:\\s?\\(|:-\\(|\\)\\s?:|\\)-:)', \" em_negative \")  # Sad -- :-(, : (, :(, ):, )-:\n",
    "    text = re_sub(r'(:,\\(|:\\'\\(|:\"\\()', \" em_negative \")  # Cry -- :,(, :'(, :\"(\n",
    "    text = re_sub(r\"(-|\\')\", \"\")  # remove &\n",
    "    text = re_sub(r\"/\", \" / \")\n",
    "    text = re_sub(r\"@[0-9]+-\", \" \")\n",
    "    text = re_sub(r\"@\\w+\", \" \")\n",
    "    text = re_sub(r\"{}{}[)dD]+|[)dD]+{}{}\".format(eyes, nose, nose, eyes), \" em_positive \")\n",
    "    text = re_sub(r\"{}{}p+\".format(eyes, nose), \" em_positive \")\n",
    "    text = re_sub(r\"{}{}\\(+|\\)+{}{}\".format(eyes, nose, nose, eyes), \" em_negative \")\n",
    "    text = re_sub(r\"{}{}[\\/|l*]\".format(eyes, nose), \" em_neutralface \")\n",
    "    text = re_sub(r\"<3\", \" heart \")\n",
    "    text = re_sub(r\"[-+]?[.\\d]*[\\d]+[:,.\\d]*\", \" \")\n",
    "    text = re_sub(r'-', r' ')\n",
    "    # text = re_sub(r\"https?:\\/\\/\\S+\\b|www\\.(\\w+\\.)+\\S*\", \"  \")\n",
    "    #     text = re_sub(r\"([pls?s]){2,}\", r\"\\1\")\n",
    "    #     text = re_sub(r\"([plz?z]){2,}\", r\"\\1\")\n",
    "    text = re_sub(r'\\\\n', r' ')\n",
    "    text = re_sub(r\" app \", \" application \")\n",
    "    text = re_sub(r\"app\", \" application\")\n",
    "    text = re_sub(r\" wil \", \" will \")\n",
    "    text = re_sub(r\" im \", \" i am \")\n",
    "    text = re_sub(r\" al \", \" all \")\n",
    "    # text = re_sub(r\"<3\",\"love\")\n",
    "    text = re_sub(r\" sx \", \" sex \")\n",
    "    text = re_sub(r\" u \", \" you \")\n",
    "    text = re_sub(r\" r \", \" are \")\n",
    "    text = re_sub(r\" y \", \" why \")\n",
    "    text = re_sub(r\" Y \", \" WHY \")\n",
    "    text = re_sub(r\"Y \", \" WHY \")\n",
    "    text = re_sub(r\" hv \", \" have \")\n",
    "    text = re_sub(r\" c \", \" see \")\n",
    "    text = re_sub(r\" bcz \", \" because \")\n",
    "    text = re_sub(r\" coz \", \" because \")\n",
    "    text = re_sub(r\" v \", \" we \")\n",
    "    text = re_sub(r\" ppl \", \" people \")\n",
    "    text = re_sub(r\" pepl \", \" people \")\n",
    "    text = re_sub(r\" r b i \", \" rbi \")\n",
    "    text = re_sub(r\" R B I \", \" RBI \")\n",
    "    text = re_sub(r\" R b i \", \" rbi \")\n",
    "    text = re_sub(r\" R \", \" ARE \")\n",
    "    text = re_sub(r\" hav \", \" have \")\n",
    "    text = re_sub(r\"R \", \" ARE \")\n",
    "    text = re_sub(r\" U \", \" you \")\n",
    "    text = re_sub(r\" üëé \", \" em_negative \")\n",
    "    text = re_sub(r\"U \", \" you \")\n",
    "    text = re_sub(r\" pls \", \" please \")\n",
    "    text = re_sub(r\"Pls \", \"Please \")\n",
    "    text = re_sub(r\"plz \", \"please \")\n",
    "    text = re_sub(r\"Plz \", \"Please \")\n",
    "    text = re_sub(r\"PLZ \", \"Please \")\n",
    "    text = re_sub(r\"Pls\", \"Please \")\n",
    "    text = re_sub(r\"plz\", \"please \")\n",
    "    text = re_sub(r\"Plz\", \"Please \")\n",
    "    text = re_sub(r\"PLZ\", \"Please \")\n",
    "    text = re_sub(r\" thankz \", \" thanks \")\n",
    "    text = re_sub(r\" thnx \", \" thanks \")\n",
    "    text = re_sub(r\"fuck\\w+ \", \" fuck \")\n",
    "    text = re_sub(r\"f\\*\\* \", \" fuck \")\n",
    "    text = re_sub(r\"\\*\\*\\*k \", \" fuck \")\n",
    "    text = re_sub(r\"F\\*\\* \", \" fuck \")\n",
    "    text = re_sub(r\"mo\\*\\*\\*\\*\\* \", \" fucker \")\n",
    "    text = re_sub(r\"b\\*\\*\\*\\* \", \" blody \")\n",
    "    text = re_sub(r\" mc \", \" fucker \")\n",
    "    text = re_sub(r\" MC \", \" fucker \")\n",
    "    text = re_sub(r\" wtf \", \" fuck \")\n",
    "    text = re_sub(r\" ch\\*\\*\\*ya \", \" fucker \")\n",
    "    text = re_sub(r\" ch\\*\\*Tya \", \" fucker \")\n",
    "    text = re_sub(r\" ch\\*\\*Tia \", \" fucker \")\n",
    "    text = re_sub(r\" C\\*\\*\\*yas \", \" fucker \")\n",
    "    text = re_sub(r\"l\\*\\*\\*\\* \", \"shit \")\n",
    "    text = re_sub(r\" A\\*\\*\\*\\*\\*\\*S\", \" ASSHOLES\")\n",
    "    text = re_sub(r\" di\\*\\*\\*\\*s\", \" cker\")\n",
    "    text = re_sub(r\" nd \", \" and \")\n",
    "    text = re_sub(r\"Nd \", \"and \")\n",
    "    text = re_sub(r\"([!?!]){2,}\", r\"! \")\n",
    "    text = re_sub(r\"([.?.]){2,}\", r\". \")\n",
    "    text = re_sub(r\"([*?*]){2,}\", r\"* \")\n",
    "    text = re_sub(r\"([,?,]){2,}\", r\", \")\n",
    "    text = re_sub(r\"ha?ha\", r\" em_positive \")\n",
    "    # text = re_sub(r\"([!]){2,}\", r\"! \")\n",
    "    # text = re_sub(r\"([.]){2,}\", r\". \")\n",
    "    # text = re_sub(r\"([*]){2,}\", r\"* \")\n",
    "    # text = re_sub(r\"([,]){2,}\", r\", \")\n",
    "    # text = re_sub(r\"\\n\\r\", \" \")\n",
    "    text = re_sub(r\"(ind[vs]pak)\", \" india versus pakistan \")\n",
    "    text = re_sub(r\"(pak[vs]ind)\", \" pakistan versus india \")\n",
    "    text = re_sub(r\"(indvsuae)\", \" india versus United Arab Emirates \")\n",
    "    text = re_sub(r\"[sS]hut[Dd]own[jnuJNU]\", \" shut down jnu \")\n",
    "    # text = re_sub(r\"ShutDownJNU\", \" shut down jnu \")\n",
    "    # text = re_sub(r\"[-+]?[.\\d]*[\\d]+[:,.\\d]*\", \" number \")\n",
    "    # text = re_sub(r\"(.)\\1\\1+\", r\"\\1\") # remove funnnnny --> funny\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c1ae1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the paths to the corpus. It should end in aclImdb/train\n",
    "CORPUS_PATH = \"C:\\\\Users\\\\morzm\\\\jup_txts\\\\corpus\\\\\"\n",
    "# Add the path to the embeddings. It should end in GoogleNews-vectors-negative300.bin.gz\n",
    "GOOGLE_VECTORS = \"C:\\\\Users\\\\morzm\\\\jup_txts\\\\GoogleNews-vectors-negative300.bin.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "611ad4a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " 'Congratulations Sharon!  @Lynden_Leigh: T minus 24 hrs I leave the corporate world for full #entrepreneurship, .. \\n')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing the data\n",
    "dataset = pre_process_data(CORPUS_PATH)\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29cd0db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to tokenise and vectorise all the training data\n",
    "\n",
    "def tokenize_and_vectorize(dataset):\n",
    "    tokenizer = TreebankWordTokenizer()\n",
    "    vectorized_data = []\n",
    "#    expected = [] this line appears in the book, but it's not necessary here!\n",
    "    for sample in dataset:\n",
    "        tokens = tokenizer.tokenize(sample[1])\n",
    "        sample_vecs = []\n",
    "        for token in tokens:\n",
    "            try:\n",
    "                sample_vecs.append(word_vectors[token])\n",
    "            except KeyError:\n",
    "                pass # No matching token in the Google w2v vocab\n",
    "        vectorized_data.append(sample_vecs)\n",
    "\n",
    "    return vectorized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45ca6825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to get the target labels\n",
    "def collect_expected(dataset):\n",
    "    \"\"\" Peel off the target values from the dataset \"\"\"\n",
    "    expected = []\n",
    "    for sample in dataset:\n",
    "        expected.append(sample[0])\n",
    "    return expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "556c11e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the word2vec embeddings\n",
    "\n",
    "word_vectors = KeyedVectors.load_word2vec_format(GOOGLE_VECTORS,\n",
    "    binary=True, limit=400000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4bb405b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "vectorized_data = tokenize_and_vectorize(dataset)\n",
    "expected = collect_expected(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0d222438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating training and validation partitions\n",
    "\n",
    "split_point = int(len(vectorized_data)*.8)\n",
    "x_train = vectorized_data[:split_point] # there's a typo in this line, if copying from the book\n",
    "y_train = expected[:split_point]\n",
    "x_test = vectorized_data[split_point:]\n",
    "y_test = expected[split_point:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "654e5e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network parameters\n",
    "\n",
    "maxlen = 70          # maximum length of the text (why?) --- number of vectors (word embeddings), playable\n",
    "batch_size = 32       # number of samples before backpropagating, usually powers of 2, playable\n",
    "embedding_dims = 300  # Same as Google's, don't play with this\n",
    "filters = 250         # (!)\n",
    "kernel_size = 3       # remember: filter=kernel (we have a scalar this time), you can play with this\n",
    "hidden_dims = 250     # number of neurons in the final layer, playable\n",
    "epochs = 3            # number of training epochs, playable\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "963a5732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to pad or truncate the input\n",
    "# (notice that this code is quite verbose)\n",
    "def pad_trunc(data, maxlen):\n",
    "    \"\"\"\n",
    "    For a given dataset pad with zero vectors or truncate to maxlen\n",
    "    \"\"\"\n",
    "    new_data = []\n",
    "    # Create a vector of 0s the length of our word vectors\n",
    "    zero_vector = []\n",
    "    for _ in range(len(data[0][0])):\n",
    "        zero_vector.append(0.0)\n",
    "\n",
    "    for sample in data:\n",
    "        if len(sample) > maxlen:\n",
    "            temp = sample[:maxlen]\n",
    "        elif len(sample) < maxlen:\n",
    "            temp = sample\n",
    "            # Append the appropriate number 0 vectors to the list\n",
    "            additional_elems = maxlen - len(sample)\n",
    "            for _ in range(additional_elems):\n",
    "                temp.append(zero_vector)\n",
    "        else:\n",
    "            temp = sample\n",
    "        new_data.append(temp)\n",
    "\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4cb13c71",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 61.3 GiB for an array with shape (391920, 70, 300) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [30]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m x_test \u001b[38;5;241m=\u001b[39m pad_trunc(x_test, maxlen)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# The shape is [number of samples, sequence length, word vector]  CUBE \u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m x_train \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxlen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_dims\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m y_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(y_train)\n\u001b[0;32m      9\u001b[0m x_test \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreshape(x_test, (\u001b[38;5;28mlen\u001b[39m(x_test), maxlen, embedding_dims))\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mreshape\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\numpy\\core\\fromnumeric.py:298\u001b[0m, in \u001b[0;36mreshape\u001b[1;34m(a, newshape, order)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_reshape_dispatcher)\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreshape\u001b[39m(a, newshape, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;124;03m    Gives a new shape to an array without changing its data.\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    296\u001b[0m \u001b[38;5;124;03m           [5, 6]])\u001b[39;00m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreshape\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\numpy\\core\\fromnumeric.py:54\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     52\u001b[0m bound \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(obj, method, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bound \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bound(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\numpy\\core\\fromnumeric.py:43\u001b[0m, in \u001b[0;36m_wrapit\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[0;32m     42\u001b[0m     wrap \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m, method)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wrap:\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, mu\u001b[38;5;241m.\u001b[39mndarray):\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 61.3 GiB for an array with shape (391920, 70, 300) and data type float64"
     ]
    }
   ],
   "source": [
    "# Padding/truncating the data (if necessary)\n",
    "\n",
    "x_train = pad_trunc(x_train, maxlen)\n",
    "x_test = pad_trunc(x_test, maxlen)\n",
    "\n",
    "# The shape is [number of samples, sequence length, word vector]  CUBE \n",
    "x_train = np.reshape(x_train, (len(x_train), maxlen, embedding_dims))\n",
    "y_train = np.array(y_train)\n",
    "x_test = np.reshape(x_test, (len(x_test), maxlen, embedding_dims))\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676f661a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
