{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4cc3a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dependencies\n",
    "\n",
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "\n",
    "import os.path\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "from random import shuffle\n",
    "from keras.preprocessing import sequence   # necessary for padding\n",
    "from keras.models import Sequential        # Base Keras NN model\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D # Convolution layer and pooling\n",
    "from keras.layers import Dense, Dropout, Activation # The objects for each layer\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from nltk.stem.porter import PorterStemmer   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2b22434",
   "metadata": {},
   "outputs": [],
   "source": [
    "xmls_directory = \"C:\\\\Users\\\\morzm\\\\jup_txts\\\\reddit\\\\r4r\\\\\" # path to reddit corpus xmls\n",
    "truth_path = \"C:\\\\Users\\\\morzm\\\\jup_txts\\\\reddit_corpus\\\\truth.txt\" # path to reddit truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b390d8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This bit of code transforms the .xml files of the reddit posts\n",
    "# into txts, depending on the title of the reddit post.\n",
    "# no gold standard. Also, there are way less women on reddit than men.\n",
    "\n",
    "\n",
    "m4m = \"M4M\"\n",
    "m4f = \"M4F\"\n",
    "m4r = \"M4R\"\n",
    "f4f = \"F4F\"\n",
    "f4m = \"F4M\"\n",
    "f4r = \"F4R\"\n",
    "n = 0\n",
    "\n",
    "for file in os.listdir(xmls_directory):\n",
    "    if m4f in file:\n",
    "        n += 1\n",
    "        save_path = \"C:\\\\Users\\\\morzm\\\\jup_txts\\\\reddit_corpus\\\\male\\\\\"\n",
    "        os.chdir(\"C:\\\\Users\\\\morzm\\\\jup_txts\\\\reddit\\\\r4r\\\\\")\n",
    "        xml_file = file\n",
    "        tree = ET.parse(xml_file)\n",
    "        root = tree.getroot()\n",
    "        with open (os.path.join(\"C:\\\\Users\\\\morzm\\\\jup_txts\\\\reddit_corpus\\\\male\\\\\", \"m4f_\"+str(n)+\".txt\"), \"w\"):\n",
    "            for self in root.findall('selftext'):\n",
    "                post = self.text\n",
    "                text = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', post)\n",
    "                line_to_write = text + '\\n'\n",
    "                with open(os.path.join(save_path, \"m4f_\"+str(n)+\".txt\"), 'a', encoding=\"utf-8\") as f:\n",
    "                    f.write(line_to_write)\n",
    "        with open (os.path.join(\"C:\\\\Users\\\\morzm\\\\jup_txts\\\\reddit_corpus\\\\\", \"truth.txt\"), \"a\") as f:\n",
    "            f.write(\"m4f_\"+str(n)+\":::male\\n\")\n",
    "                    \n",
    "    elif m4m in file:\n",
    "        n += 1\n",
    "        save_path = \"C:\\\\Users\\\\morzm\\\\jup_txts\\\\reddit_corpus\\\\male\\\\\"\n",
    "        os.chdir(\"C:\\\\Users\\\\morzm\\\\jup_txts\\\\reddit\\\\r4r\\\\\")\n",
    "        xml_file = file\n",
    "        tree = ET.parse(xml_file)\n",
    "        root = tree.getroot()\n",
    "        with open (os.path.join(\"C:\\\\Users\\\\morzm\\\\jup_txts\\\\reddit_corpus\\\\male\\\\\", \"m4m_\"+str(n)+\".txt\"), \"w\"):\n",
    "            for posts in root.findall('selftext'):\n",
    "                post = posts.text\n",
    "                text = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', post)\n",
    "                line_to_write = text + '\\n'\n",
    "                with open(os.path.join(save_path, \"m4m_\"+str(n)+\".txt\"), 'a', encoding=\"utf-8\") as f:\n",
    "                    f.write(line_to_write)\n",
    "        with open (os.path.join(\"C:\\\\Users\\\\morzm\\\\jup_txts\\\\reddit_corpus\\\\\", \"truth.txt\"), \"a\") as f:\n",
    "            f.write(\"m4m_\"+str(n)+\":::male\\n\")\n",
    "            \n",
    "    elif m4r in file:\n",
    "        n += 1\n",
    "        save_path = \"C:\\\\Users\\\\morzm\\\\jup_txts\\\\reddit_corpus\\\\male\\\\\"\n",
    "        os.chdir(\"C:\\\\Users\\\\morzm\\\\jup_txts\\\\reddit\\\\r4r\\\\\")\n",
    "        xml_file = file\n",
    "        tree = ET.parse(xml_file)\n",
    "        root = tree.getroot()\n",
    "        with open (os.path.join(\"C:\\\\Users\\\\morzm\\\\jup_txts\\\\reddit_corpus\\\\male\\\\\", \"m4r_\"+str(n)+\".txt\"), \"w\"):\n",
    "            for posts in root.findall('selftext'):\n",
    "                post = posts.text\n",
    "                text = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', post)\n",
    "                line_to_write = text + '\\n'\n",
    "                with open(os.path.join(save_path, \"m4r_\"+str(n)+\".txt\"), 'a', encoding=\"utf-8\") as f:\n",
    "                    f.write(line_to_write)\n",
    "        with open (os.path.join(\"C:\\\\Users\\\\morzm\\\\jup_txts\\\\reddit_corpus\\\\\", \"truth.txt\"), \"a\") as f:\n",
    "            f.write(\"m4r_\"+str(n)+\":::male\\n\")\n",
    "                    \n",
    "    elif f4m in file:\n",
    "        n += 1\n",
    "        save_path = \"C:\\\\Users\\\\morzm\\\\jup_txts\\\\reddit_corpus\\\\female\\\\\"\n",
    "        os.chdir(\"C:\\\\Users\\\\morzm\\\\jup_txts\\\\reddit\\\\r4r\\\\\")\n",
    "        xml_file = file\n",
    "        tree = ET.parse(xml_file)\n",
    "        root = tree.getroot()\n",
    "        with open (os.path.join(\"C:\\\\Users\\\\morzm\\\\jup_txts\\\\reddit_corpus\\\\female\\\\\", \"f4m_\"+str(n)+\".txt\"), \"w\"):\n",
    "            for posts in root.findall('selftext'):\n",
    "                post = posts.text\n",
    "                text = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', post)\n",
    "                line_to_write = text + '\\n'\n",
    "                with open(os.path.join(save_path, \"f4m_\"+str(n)+\".txt\"), 'a', encoding=\"utf-8\") as f:\n",
    "                    f.write(line_to_write)\n",
    "        with open (os.path.join(\"C:\\\\Users\\\\morzm\\\\jup_txts\\\\reddit_corpus\\\\\", \"truth.txt\"), \"a\") as f:\n",
    "            f.write(\"f4m_\"+str(n)+\":::female\\n\")\n",
    "    \n",
    "    elif f4f in file:\n",
    "        n += 1\n",
    "        save_path = \"C:\\\\Users\\\\morzm\\\\jup_txts\\\\reddit_corpus\\\\female\\\\\"\n",
    "        os.chdir(\"C:\\\\Users\\\\morzm\\\\jup_txts\\\\reddit\\\\r4r\\\\\")\n",
    "        xml_file = file\n",
    "        tree = ET.parse(xml_file)\n",
    "        root = tree.getroot()\n",
    "        with open (os.path.join(\"C:\\\\Users\\\\morzm\\\\jup_txts\\\\reddit_corpus\\\\female\\\\\", \"f4f_\"+str(n)+\".txt\"), \"w\"):\n",
    "            for posts in root.findall('selftext'):\n",
    "                post = posts.text\n",
    "                text = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', post)\n",
    "                line_to_write = text + '\\n'\n",
    "                with open(os.path.join(save_path, \"f4f_\"+str(n)+\".txt\"), 'a', encoding=\"utf-8\") as f:\n",
    "                    f.write(line_to_write)\n",
    "        with open (os.path.join(\"C:\\\\Users\\\\morzm\\\\jup_txts\\\\reddit_corpus\\\\\", \"truth.txt\"), \"a\") as f:\n",
    "            f.write(\"f4f_\"+str(n)+\":::female\\n\")\n",
    "            \n",
    "    elif f4r in file:\n",
    "        n += 1\n",
    "        save_path = \"C:\\\\Users\\\\morzm\\\\jup_txts\\\\reddit_corpus\\\\female\\\\\"\n",
    "        os.chdir(\"C:\\\\Users\\\\morzm\\\\jup_txts\\\\reddit\\\\r4r\\\\\")\n",
    "        xml_file = file\n",
    "        tree = ET.parse(xml_file)\n",
    "        root = tree.getroot()\n",
    "        with open (os.path.join(\"C:\\\\Users\\\\morzm\\\\jup_txts\\\\reddit_corpus\\\\female\\\\\", \"f4r_\"+str(n)+\".txt\"), \"w\"):\n",
    "            for posts in root.findall('selftext'):\n",
    "                post = posts.text\n",
    "                text = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', post)\n",
    "                line_to_write = text + '\\n'\n",
    "                with open(os.path.join(save_path, \"f4r_\"+str(n)+\".txt\"), 'a', encoding=\"utf-8\") as f:\n",
    "                    f.write(line_to_write)\n",
    "        with open (os.path.join(\"C:\\\\Users\\\\morzm\\\\jup_txts\\\\reddit_corpus\\\\\", \"truth.txt\"), \"a\") as f:\n",
    "            f.write(\"f4r_\"+str(n)+\":::female\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6018852",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import model_from_json\n",
    "\n",
    "json_file = open(\"C:\\\\Users\\\\morzm\\\\Jupyter\\\\models\\\\fasttext_cnn_model.json\", 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"C:\\\\Users\\\\morzm\\\\Jupyter\\\\models\\\\fasttext_cnn_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0656b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_18 (Conv1D)          (None, 598, 250)          225250    \n",
      "                                                                 \n",
      " global_max_pooling1d_18 (Gl  (None, 250)              0         \n",
      " obalMaxPooling1D)                                               \n",
      "                                                                 \n",
      " dense_36 (Dense)            (None, 250)               62750     \n",
      "                                                                 \n",
      " dropout_18 (Dropout)        (None, 250)               0         \n",
      "                                                                 \n",
      " activation_36 (Activation)  (None, 250)               0         \n",
      "                                                                 \n",
      " dense_37 (Dense)            (None, 1)                 251       \n",
      "                                                                 \n",
      " activation_37 (Activation)  (None, 1)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 288,251\n",
      "Trainable params: 288,251\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f27f2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO STEMMING AND STOPWORDS\n",
    "\n",
    "my_word_stop = ['the', 'in', 'of', 'is', 'a', 'to', 'an', 'be']\n",
    "\n",
    "def tokenize_and_vectorize(dataset):\n",
    "    tokenizer = TreebankWordTokenizer()\n",
    "    vectorized_data = []\n",
    "\n",
    "    for sample in dataset:\n",
    "        lowercase = sample[1].lower()\n",
    "        tokens = tokenizer.tokenize(lowercase)\n",
    "        sample_vecs = []\n",
    "        for token in tokens:\n",
    "            if token not in my_word_stop:\n",
    "                #print(token)\n",
    "                try:\n",
    "                    sample_vecs.append(word_vectors[token])\n",
    "                except KeyError:\n",
    "                    pass # No matching token in the Google w2v vocab\n",
    "        vectorized_data.append(sample_vecs)\n",
    "\n",
    "    return vectorized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74f73e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load this data\n",
    "\n",
    "def pre_process_data(filepath):\n",
    "    male_path = os.path.join(filepath, 'male')\n",
    "    female_path = os.path.join(filepath, 'female')\n",
    "    male_label = 1\n",
    "    female_label = 0\n",
    "    dataset = []\n",
    "    for filename in glob.glob(os.path.join(male_path, '*.txt')):\n",
    "        with open(filename, 'r', encoding=\"utf-8\") as f:\n",
    "            dataset.append((male_label, f.read()))\n",
    "    for filename in glob.glob(os.path.join(female_path, '*.txt')):\n",
    "        with open(filename, 'r', encoding=\"utf-8\") as f:\n",
    "            dataset.append((female_label, f.read()))\n",
    "    shuffle(dataset)\n",
    "    return dataset\n",
    "\n",
    "def collect_expected(dataset):\n",
    "    return [sample[0] for sample in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14c3bad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to get the target labels\n",
    "def collect_expected(dataset):\n",
    "    \"\"\" Peel off the target values from the dataset \"\"\"\n",
    "    expected = []\n",
    "    for sample in dataset:\n",
    "        expected.append(sample[0])\n",
    "    return expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f04cec1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to pad or truncate the input\n",
    "# (notice that this code is quite verbose)\n",
    "def pad_trunc(data, maxlen):\n",
    "    \"\"\"\n",
    "    For a given dataset pad with zero vectors or truncate to maxlen\n",
    "    \"\"\"\n",
    "    new_data = []\n",
    "    # Create a vector of 0s the length of our word vectors\n",
    "    zero_vector = []\n",
    "    for _ in range(len(data[0][0])):\n",
    "        zero_vector.append(0.0)\n",
    "\n",
    "    for sample in data:\n",
    "        if len(sample) > maxlen:\n",
    "            temp = sample[:maxlen]\n",
    "        elif len(sample) < maxlen:\n",
    "            temp = sample\n",
    "            # Append the appropriate number 0 vectors to the list\n",
    "            additional_elems = maxlen - len(sample)\n",
    "            for _ in range(additional_elems):\n",
    "                temp.append(zero_vector)\n",
    "        else:\n",
    "            temp = sample\n",
    "        new_data.append(temp)\n",
    "\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e4df47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# google_vectors = \"C:\\\\Users\\\\morzm\\\\jup_txts\\\\GoogleNews-vectors-negative300.bin.gz\"\n",
    "# # Loading the word2vec embeddings\n",
    "\n",
    "# word_vectors = KeyedVectors.load_word2vec_format(google_vectors,\n",
    "#     binary=True, limit=400000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ead295a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the fasttext embeddings\n",
    "\n",
    "fasttext_vectors = 'crawl-300d-2M.vec'\n",
    "word_vectors = KeyedVectors.load_word2vec_format(fasttext_vectors, binary=False, limit=400000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6c53af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 600\n",
    "embedding_dims = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0714b1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5532177]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicting a new instance\n",
    "\n",
    "sample_1 = \"C:\\\\Users\\\\morzm\\\\jup_txts\\\\reddit\\\\female\\\\f1.txt\"\n",
    "\n",
    "vec_list = tokenize_and_vectorize([(1, sample_1)])\n",
    "test_vec_list = pad_trunc(vec_list, maxlen)\n",
    "test_vec = np.reshape(test_vec_list, (len(test_vec_list), maxlen,\\\n",
    "        embedding_dims))\n",
    "loaded_model.predict(test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f972848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Male\n"
     ]
    }
   ],
   "source": [
    "# Get the class\n",
    "if loaded_model.predict(test_vec) > 0.5:\n",
    "    print(\"Male\")\n",
    "else:\n",
    "    print(\"Female\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "866dba51",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_male_path = \"C:\\\\Users\\\\morzm\\\\jup_txts\\\\reddit_corpus\\\\male\\\\\"\n",
    "corpus_female_path = \"C:\\\\Users\\\\morzm\\\\jup_txts\\\\reddit_corpus\\\\female\\\\\"\n",
    "corpus_path = \"C:\\\\Users\\\\morzm\\\\jup_txts\\\\reddit_corpus\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3502647b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 out of 1429\n",
      "100 out of 1429\n",
      "200 out of 1429\n",
      "300 out of 1429\n",
      "400 out of 1429\n",
      "500 out of 1429\n",
      "600 out of 1429\n",
      "700 out of 1429\n",
      "800 out of 1429\n",
      "900 out of 1429\n",
      "1000 out of 1429\n",
      "1100 out of 1429\n",
      "1200 out of 1429\n",
      "1300 out of 1429\n",
      "1400 out of 1429\n",
      "60.461861441567535 % accuracy\n",
      "377 true males\n",
      "367 false males\n",
      "out of 744\n",
      "487 true females\n",
      "198 false females\n",
      "out of 685\n"
     ]
    }
   ],
   "source": [
    "# This code tests the model on the pre-existing 'testing' subset of the PAN2018 database\n",
    "# Can be tweaked to test on any data\n",
    "\n",
    "n_total = 0 # total instances\n",
    "pred_n = 0 # correctly predicted instances\n",
    "true_pos = 0\n",
    "false_pos = 0\n",
    "true_neg = 0\n",
    "false_neg = 0\n",
    "mal = 0\n",
    "fem = 0\n",
    "\n",
    "with open(truth_path, 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        if n_total % 100 == 0:\n",
    "            print(n_total, \"out of 1429\")\n",
    "        string = line.partition(\":::\")\n",
    "        if \"male\\n\" in string:\n",
    "            author_id = string[0] # need this to open files\n",
    "            author_gender = \"male\"\n",
    "            for file in os.listdir(corpus_male_path):\n",
    "                if file.endswith(author_id+\".txt\"):\n",
    "                    with open(\"C:\\\\Users\\\\morzm\\\\jup_txts\\\\reddit_corpus\\\\male\\\\\"+author_id+\".txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "                        testfile = f.readlines()\n",
    "                        vec_list = tokenize_and_vectorize([(1, str(testfile))])\n",
    "                        test_vec_list = pad_trunc(vec_list, maxlen)\n",
    "                        test_vec = np.reshape(test_vec_list, (len(test_vec_list), maxlen,\\\n",
    "                                embedding_dims))\n",
    "                        loaded_model.predict(test_vec)                   \n",
    "   \n",
    "                        if loaded_model.predict(test_vec) > 0.5:\n",
    "                            predicted_gender = \"male\"\n",
    "                        else:\n",
    "                            predicted_gender = \"female\"\n",
    "                \n",
    "                        if predicted_gender == author_gender:\n",
    "                            n_total += 1\n",
    "                            pred_n += 1\n",
    "                            true_pos += 1\n",
    "                            mal += 1\n",
    "                        else:\n",
    "                            n_total += 1\n",
    "                            false_pos += 1\n",
    "                            mal += 1\n",
    "            \n",
    "\n",
    "                              \n",
    "        elif \"female\\n\" in string:\n",
    "            author_id = string[0]\n",
    "            author_gender = \"female\"\n",
    "            for file in os.listdir(corpus_female_path):\n",
    "                if file.endswith(author_id+\".txt\"):\n",
    "                    with open(\"C:\\\\Users\\\\morzm\\\\jup_txts\\\\reddit_corpus\\\\female\\\\\"+author_id+\".txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "                        testfile = f.readlines()\n",
    "                        vec_list = tokenize_and_vectorize([(1, str(testfile))])\n",
    "                        test_vec_list = pad_trunc(vec_list, maxlen)\n",
    "                        test_vec = np.reshape(test_vec_list, (len(test_vec_list), maxlen,\\\n",
    "                                embedding_dims))\n",
    "                        loaded_model.predict(test_vec)\n",
    "                    \n",
    "                        if loaded_model.predict(test_vec) > 0.5:\n",
    "                            predicted_gender = \"male\"\n",
    "                        else:\n",
    "                            predicted_gender = \"female\"\n",
    "                \n",
    "                        if predicted_gender == author_gender:\n",
    "                            n_total += 1\n",
    "                            pred_n += 1\n",
    "                            true_neg += 1\n",
    "                            fem += 1\n",
    "\n",
    "                        else:\n",
    "                            n_total += 1\n",
    "                            false_neg += 1\n",
    "                            fem += 1\n",
    "                    \n",
    "    sum = pred_n/n_total\n",
    "    print(sum*100, \"% accuracy\")   # accuracy %\n",
    "    print(true_pos, \"true males\")\n",
    "    print(false_pos, \"false males\")\n",
    "    print (\"out of\", mal)\n",
    "    print(true_neg, \"true females\")\n",
    "    print(false_neg, \"false females\")\n",
    "    print(\"out of\", fem)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3f53c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 out of 1429\n",
      "0 out of 1429\n",
      "0 out of 1429\n",
      "100 out of 1429\n",
      "100 out of 1429\n",
      "200 out of 1429\n",
      "300 out of 1429\n",
      "300 out of 1429\n",
      "400 out of 1429\n",
      "500 out of 1429\n",
      "600 out of 1429\n",
      "71.0948905109489\n"
     ]
    }
   ],
   "source": [
    "# This code tests the model on the pre-existing 'testing' subset of the PAN2018 database\n",
    "# Can be tweaked to test on any data\n",
    "\n",
    "n_total = 0 # total instances\n",
    "pred_n = 0 # correctly predicted instances\n",
    "true_pos = 0\n",
    "false_pos = 0\n",
    "\n",
    "with open(truth_path, 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        if n_total % 100 == 0:\n",
    "            print(n_total, \"out of 1429\")\n",
    "        string = line.partition(\":::\")\n",
    "        if \"male\\n\" in string:\n",
    "            i = 0\n",
    "#             author_id = string[0] # need this to open files\n",
    "#             author_gender = \"male\"\n",
    "#             for file in os.listdir(corpus_male_path):\n",
    "#                 if file.endswith(author_id+\".txt\"):\n",
    "#                     with open(\"C:\\\\Users\\\\morzm\\\\jup_txts\\\\reddit_corpus\\\\male\\\\\"+author_id+\".txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "#                         testfile = f.readlines()\n",
    "#                         vec_list = tokenize_and_vectorize([(1, str(testfile))])\n",
    "#                         test_vec_list = pad_trunc(vec_list, maxlen)\n",
    "#                         test_vec = np.reshape(test_vec_list, (len(test_vec_list), maxlen,\\\n",
    "#                                 embedding_dims))\n",
    "#                         loaded_model.predict(test_vec)                   \n",
    "   \n",
    "#                         if loaded_model.predict(test_vec) > 0.5:\n",
    "#                             predicted_gender = \"male\"\n",
    "#                         else:\n",
    "#                             predicted_gender = \"female\"\n",
    "                \n",
    "#                         if predicted_gender == author_gender:\n",
    "#                             n_total += 1\n",
    "#                             pred_n += 1\n",
    "#                         else:\n",
    "#                             n_total += 1\n",
    "            \n",
    "\n",
    "                              \n",
    "        elif \"female\\n\" in string:\n",
    "            author_id = string[0]\n",
    "            author_gender = \"female\"\n",
    "            for file in os.listdir(corpus_female_path):\n",
    "                if file.endswith(author_id+\".txt\"):\n",
    "                    with open(\"C:\\\\Users\\\\morzm\\\\jup_txts\\\\reddit_corpus\\\\female\\\\\"+author_id+\".txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "                        testfile = f.readlines()\n",
    "                        vec_list = tokenize_and_vectorize([(1, str(testfile))])\n",
    "                        test_vec_list = pad_trunc(vec_list, maxlen)\n",
    "                        test_vec = np.reshape(test_vec_list, (len(test_vec_list), maxlen,\\\n",
    "                                embedding_dims))\n",
    "                        loaded_model.predict(test_vec)\n",
    "                    \n",
    "                        if loaded_model.predict(test_vec) > 0.5:\n",
    "                            predicted_gender = \"male\"\n",
    "                        else:\n",
    "                            predicted_gender = \"female\"\n",
    "                \n",
    "                        if predicted_gender == author_gender:\n",
    "                            n_total += 1\n",
    "                            pred_n += 1\n",
    "\n",
    "                        else:\n",
    "                            n_total += 1\n",
    "                    \n",
    "    sum = pred_n/n_total\n",
    "    print(sum*100)   # accuracy %\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8983594",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
