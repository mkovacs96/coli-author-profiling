{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ef0b205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dependencies\n",
    "\n",
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "\n",
    "import os.path\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "from random import shuffle\n",
    "from keras.preprocessing import sequence   # necessary for padding\n",
    "from keras.models import Sequential        # Base Keras NN model\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D # Convolution layer and pooling\n",
    "from keras.layers import Dense, Dropout, Activation # The objects for each layer\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from gensim.models.keyedvectors import KeyedVectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6caf543f",
   "metadata": {},
   "outputs": [],
   "source": [
    "xmls_directory = \"C:\\\\Users\\\\morzm\\\\jup_txts\\\\text\\\\\" # This is the path to the .xml files of the PAN2018 twitter corpus\n",
    "\n",
    "truth_path = \"C:\\\\Users\\\\morzm\\\\jup_txts\\\\en.txt\" # This is the path to the .txt file containing the ids and genders of each twitter user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bce04ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This bit of code transforms the .xml files containing the tweets\n",
    "# on the base of a truth file into .txts, depending on gender\n",
    "# thus creating a corpus\n",
    "\n",
    "with open(truth_path, 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        string = line.partition(\":::\") # This line divides the author_id from the gender of the author\n",
    "        if \"male\\n\" in string:\n",
    "            save_path = \"C:\\\\Users\\\\morzm\\\\jup_txts\\\\merged_corpus\\\\male\\\\\"\n",
    "            author_id = string[0]\n",
    "            for file in os.listdir(xmls_directory):\n",
    "                if file.endswith(author_id+\".xml\"):\n",
    "                    os.chdir(\"C:\\\\Users\\\\morzm\\\\jup_txts\\\\text\\\\\")\n",
    "                    xml_file = file # so that the file can be parsed by ElementTree\n",
    "                    tree = ET.parse(xml_file)\n",
    "                    root = tree.getroot()\n",
    "                    \n",
    "                    with open(os.path.join(\"C:\\\\Users\\\\morzm\\\\jup_txts\\\\merged_corpus\\\\male\\\\\", author_id+\".txt\"), \"w\"):\n",
    "                        for tweets in root.find('documents'):\n",
    "                            tweet = tweets.text\n",
    "                            text = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', tweet) # removing hyperlinks\n",
    "                            line_to_write = text + '\\n'\n",
    "                            with open(os.path.join(save_path, author_id+\".txt\"), 'a', encoding=\"utf-8\") as f:\n",
    "                              f.write(line_to_write)\n",
    "                              \n",
    "        elif \"female\\n\" in string: # same for female authors\n",
    "            save_path = \"C:\\\\Users\\\\morzm\\\\jup_txts\\\\merged_corpus\\\\female\\\\\"\n",
    "            author_id = string[0]\n",
    "            for file in os.listdir(xmls_directory):\n",
    "                if file.endswith(author_id+\".xml\"):\n",
    "                    os.chdir(\"C:\\\\Users\\\\morzm\\\\jup_txts\\\\text\\\\\")\n",
    "                    xml_file = file\n",
    "                    tree = ET.parse(xml_file)\n",
    "                    root = tree.getroot()\n",
    "                    \n",
    "                    with open(os.path.join(\"C:\\\\Users\\\\morzm\\\\jup_txts\\\\merged_corpus\\\\female\\\\\", author_id+\".txt\"), \"w\"):\n",
    "                        for tweets in root.find('documents'):\n",
    "                            tweet = tweets.text\n",
    "                            text = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', tweet)\n",
    "                            line_to_write = text + '\\n'\n",
    "                            with open(os.path.join(save_path, author_id+\".txt\"), 'a', encoding=\"utf-8\") as f:\n",
    "                              f.write(line_to_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "853b0392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load this data\n",
    "\n",
    "def pre_process_data(filepath):\n",
    "    male_path = os.path.join(filepath, 'male')\n",
    "    female_path = os.path.join(filepath, 'female')\n",
    "    male_label = 1\n",
    "    female_label = 0\n",
    "    dataset = []\n",
    "    for filename in glob.glob(os.path.join(male_path, '*.txt')):\n",
    "        with open(filename, 'r', encoding=\"utf-8\") as f:\n",
    "            dataset.append((male_label, f.read()))\n",
    "    for filename in glob.glob(os.path.join(female_path, '*.txt')):\n",
    "        with open(filename, 'r', encoding=\"utf-8\") as f:\n",
    "            dataset.append((female_label, f.read()))\n",
    "    shuffle(dataset)\n",
    "    return dataset\n",
    "\n",
    "def collect_expected(dataset):\n",
    "    return [sample[0] for sample in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35aee76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_path = \"C:\\\\Users\\\\morzm\\\\jup_txts\\\\merged_corpus\\\\\"\n",
    "google_vectors = \"C:\\\\Users\\\\morzm\\\\jup_txts\\\\GoogleNews-vectors-negative300.bin.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d78a9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the data\n",
    "dataset = pre_process_data(corpus_path)\n",
    "#dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "316cfce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the word2vec embeddings\n",
    "\n",
    "word_vectors = KeyedVectors.load_word2vec_format(google_vectors,\n",
    "    binary=True, limit=400000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee1f0aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to tokenise and vectorise all the training data\n",
    "# think of a new way to tokenize\n",
    "# What about stopwords??\n",
    "\n",
    "def tokenize_and_vectorize(dataset):\n",
    "    tokenizer = TreebankWordTokenizer()\n",
    "    vectorized_data = []\n",
    "#    expected = [] this line appears in the book, but it's not necessary here!\n",
    "    for sample in dataset:\n",
    "        tokens = tokenizer.tokenize(sample[1])\n",
    "        sample_vecs = []\n",
    "        for token in tokens:\n",
    "            try:\n",
    "                sample_vecs.append(word_vectors[token])\n",
    "            except KeyError:\n",
    "                pass # No matching token in the Google w2v vocab\n",
    "        vectorized_data.append(sample_vecs)\n",
    "\n",
    "    return vectorized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec2cedf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to get the target labels\n",
    "def collect_expected(dataset):\n",
    "    \"\"\" Peel off the target values from the dataset \"\"\"\n",
    "    expected = []\n",
    "    for sample in dataset:\n",
    "        expected.append(sample[0])\n",
    "    return expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6cced169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "vectorized_data = tokenize_and_vectorize(dataset)\n",
    "expected = collect_expected(dataset)\n",
    "\n",
    "#vectorized_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8797f033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n",
      "2100\n",
      "2550\n"
     ]
    }
   ],
   "source": [
    "# Creating partitions\n",
    "# 70% train | 15% validate | 15% test\n",
    "\n",
    "split_point = int(len(vectorized_data)*.7)\n",
    "further_split = int(split_point + len(vectorized_data)*.15)\n",
    "print(len(vectorized_data))\n",
    "print(split_point)\n",
    "print(further_split)\n",
    "x_train = vectorized_data[:split_point]\n",
    "y_train = expected[:split_point]\n",
    "x_val = vectorized_data[split_point:further_split]\n",
    "y_val = expected[split_point:further_split]\n",
    "x_test = vectorized_data[further_split:]\n",
    "y_test = expected[further_split:]\n",
    "# should work. go from 0 to a then from a to b then from b to end\n",
    "\n",
    "# FOR REFERENCE\n",
    "# split_point = int(len(vectorized_data)*.7)\n",
    "# x_train = vectorized_data[:split_point] # there's a typo in this line, if copying from the book\n",
    "# y_train = expected[:split_point]\n",
    "# x_test = vectorized_data[split_point:]\n",
    "# y_test = expected[split_point:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da74cd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network parameters\n",
    "\n",
    "maxlen = 400          # think some more about this\n",
    "batch_size = 32\n",
    "embedding_dims = 300  # Same as Google's\n",
    "filters = 250\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "epochs = 3            # 2 or 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b35ed1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to pad or truncate the input\n",
    "# (notice that this code is quite verbose)\n",
    "def pad_trunc(data, maxlen):\n",
    "    \"\"\"\n",
    "    For a given dataset pad with zero vectors or truncate to maxlen\n",
    "    \"\"\"\n",
    "    new_data = []\n",
    "    # Create a vector of 0s the length of our word vectors\n",
    "    zero_vector = []\n",
    "    for _ in range(len(data[0][0])):\n",
    "        zero_vector.append(0.0)\n",
    "\n",
    "    for sample in data:\n",
    "        if len(sample) > maxlen:\n",
    "            temp = sample[:maxlen]\n",
    "        elif len(sample) < maxlen:\n",
    "            temp = sample\n",
    "            # Append the appropriate number 0 vectors to the list\n",
    "            additional_elems = maxlen - len(sample)\n",
    "            for _ in range(additional_elems):\n",
    "                temp.append(zero_vector)\n",
    "        else:\n",
    "            temp = sample\n",
    "        new_data.append(temp)\n",
    "\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16c37297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding/truncating the data (if necessary)\n",
    "\n",
    "x_train = pad_trunc(x_train, maxlen)\n",
    "x_val = pad_trunc(x_val, maxlen)\n",
    "x_test = pad_trunc(x_test, maxlen)\n",
    "\n",
    "# The shape is [number of samples, sequence length, word vector]  CUBE \n",
    "x_train = np.reshape(x_train, (len(x_train), maxlen, embedding_dims))\n",
    "y_train = np.array(y_train)\n",
    "x_val = np.reshape(x_val, (len(x_val), maxlen, embedding_dims))\n",
    "y_val = np.array(y_val)\n",
    "x_test = np.reshape(x_test, (len(x_test), maxlen, embedding_dims))\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b27b56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n"
     ]
    }
   ],
   "source": [
    "print('Building model...')\n",
    "model = Sequential()   # The standard NN model\n",
    "model.add(Conv1D(      # Adding a convolutional layer\n",
    "    filters,\n",
    "    kernel_size,\n",
    "    padding='valid',   # in this example the output is going to be lightly smaller\n",
    "    activation='relu',\n",
    "    strides=1,         # the shift\n",
    "    input_shape=(maxlen, embedding_dims))\n",
    "    )\n",
    "\n",
    "#model.summary()\n",
    "# Formulation: max (0, dot(filter, 3-gram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b1a6fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the max pooling. What is max pooling? For each filter one new version of the instance is produced, Pooling evenly divides the output of each filter into subsections\n",
    "#It selects (or computes) a representative value for each subsection.\n",
    "# Alternatives \n",
    "# - GlobalMaxPooling1D() (the max for the entire filter's output)\n",
    "# - MaxPooling1D(n)  (the max for a specific area of n; default n=2)\n",
    "# - AvgPooling1D(n)\n",
    "\n",
    "model.add(GlobalMaxPooling1D())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "114197ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding dropout (20% of the data will be \"cancelled\")\n",
    "model.add(Dense(hidden_dims))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eef892bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The classification layer!\n",
    "# sigmoid range: [0,1]\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "228a4c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d (Conv1D)             (None, 398, 250)          225250    \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 250)              0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 250)               62750     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 250)               0         \n",
      "                                                                 \n",
      " activation (Activation)     (None, 250)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 251       \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 1)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 288,251\n",
      "Trainable params: 288,251\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Compiling the CNN\n",
    "model.compile(loss='binary_crossentropy',\n",
    "        optimizer='adam',   # don't question it, just use adam\n",
    "        metrics=['accuracy']\n",
    "        )\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b455fd3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "66/66 [==============================] - 6s 69ms/step - loss: 0.6733 - accuracy: 0.5957 - val_loss: 0.7012 - val_accuracy: 0.5489\n",
      "Epoch 2/3\n",
      "66/66 [==============================] - 4s 62ms/step - loss: 0.5145 - accuracy: 0.7738 - val_loss: 0.5738 - val_accuracy: 0.6978\n",
      "Epoch 3/3\n",
      "66/66 [==============================] - 4s 62ms/step - loss: 0.2934 - accuracy: 0.9114 - val_loss: 0.5624 - val_accuracy: 0.7089\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c9cc6b1240>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting (training) the model\n",
    "model.fit(x_train, y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=(x_val, y_val)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f673326a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on test data\n",
      "4/4 [==============================] - 1s 70ms/step - loss: 0.5536 - accuracy: 0.7022\n",
      "test loss, test acc: [0.5535967946052551, 0.7022222280502319]\n",
      "Generate predictions for 3 samples\n",
      "predictions shape: (3, 1)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test data using `evaluate`\n",
    "print(\"Evaluate on test data\")\n",
    "results = model.evaluate(x_test, y_test, batch_size=128)\n",
    "print(\"test loss, test acc:\", results)\n",
    "\n",
    "# Generate predictions (probabilities -- the output of the last layer)\n",
    "# on new data using `predict`\n",
    "print(\"Generate predictions for 3 samples\")\n",
    "predictions = model.predict(x_test[:3])\n",
    "print(\"predictions shape:\", predictions.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "99c01c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "model_structure = model.to_json()\n",
    "with open(\"mf_cnn_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_structure)  # saves just the architecture\n",
    "model.save_weights(\"mf_cnn_weights.h5\")  # saves the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "02d0a893",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5749762]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicting a new instance\n",
    "\n",
    "sample_1 = \"C:\\\\Users\\\\morzm\\\\jup_txts\\\\corpus\\\\male\\\\aa4b605f6679148ff186c46a616bfe8a.txt\"\n",
    "\n",
    "vec_list = tokenize_and_vectorize([(1, sample_1)])\n",
    "test_vec_list = pad_trunc(vec_list, maxlen)\n",
    "test_vec = np.reshape(test_vec_list, (len(test_vec_list), maxlen,\\\n",
    "        embedding_dims))\n",
    "model.predict(test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aefa7301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      "Male\n"
     ]
    }
   ],
   "source": [
    "# Get the class\n",
    "print((model.predict(test_vec) > 0.5).astype(\"int32\"))\n",
    "\n",
    "if model.predict(test_vec) > 0.5:\n",
    "    print(\"Male\")\n",
    "else:\n",
    "    print(\"Female\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006e08bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
