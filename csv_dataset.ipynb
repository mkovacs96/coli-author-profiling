{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8ebab24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import os.path\n",
    "import xml.etree.ElementTree as etree\n",
    "import csv\n",
    "import getopt\n",
    "import sys\n",
    "import json\n",
    "import copy\n",
    "import tensorflow.keras.utils as TKU\n",
    "from random import shuffle\n",
    "from sklearn import utils\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import keras\n",
    "import keras.preprocessing.text as kpt\n",
    "import nltk\n",
    "from keras.models import Sequential\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Dense, Dropout, Activation, LSTM, Input, Embedding\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Input, Dense, concatenate, Activation, Average\n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "from keras.callbacks import CSVLogger\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89edf6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDir = 'C:\\\\Users\\\\morzm\\\\jup_txts\\\\csv'\n",
    "outputDir = 'C:\\\\Users\\\\morzm\\\\jup_txts\\\\csv\\\\output'\n",
    "modelDir = 'C:\\\\Users\\\\morzm\\\\jup_txts\\\\csv\\\\model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5051ed00",
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_path = \"C:\\\\Users\\\\morzm\\\\jup_txts\\\\csv\\\\en\\\\text\\\\\"\n",
    "label_path = \"C:\\\\Users\\\\morzm\\\\jup_txts\\\\csv\\\\en\\\\truth.txt\"\n",
    "label_file = open(label_path)\n",
    "model_path = os.path.join(modelDir, 'en_model.json')\n",
    "dic_path = os.path.join(modelDir, 'en_dictionary.json')\n",
    "twit_path = os.path.join(modelDir, 'twitter_en_data_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d255f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(twit_path, 'w+', encoding='utf-8') as csvfile:\n",
    "    fieldnames = ['text', 'gender']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for filename in os.listdir(xml_path):\n",
    "        if not filename.endswith('.xml'): continue\n",
    "        # ----reading file name\n",
    "        find_file_name = os.path.splitext(filename)[0]\n",
    "        # print(find_file_name)\n",
    "        # ----finding author gender\n",
    "        label_file = open(label_path)\n",
    "        for row in label_file:\n",
    "            if find_file_name in row:\n",
    "                # print(find_file_name)\n",
    "                # print(row.split(':::')[1])\n",
    "                gender = row.split(':::')[1]\n",
    "\n",
    "        # print(gender)\n",
    "\n",
    "        # ----reading xml full path\n",
    "        xml_fullname = os.path.join(xml_path, filename)\n",
    "        tree = etree.parse(xml_fullname)\n",
    "        root = tree.getroot()\n",
    "        # ----reading xml file data\n",
    "        for i in range(len(root[0])):\n",
    "            writer.writerow({'text': str(root[0][i].text), 'gender': str(gender)})\n",
    "csvfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd01bebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(word):\n",
    "    regexp = r'^(.*?)(ing|ious|ment)?$'\n",
    "    stem, suffix = re.findall(regexp, word)[0]\n",
    "    return stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38c018b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    # Different regex parts for smiley faces\n",
    "    eyes = r\"[8:=;]\"\n",
    "    nose = r\"['`\\-]?\"\n",
    "\n",
    "    # function so code less repetitive\n",
    "    def re_sub(pattern, repl):\n",
    "        return re.sub(pattern, repl, text, flags=FLAGS)\n",
    "\n",
    "    # text = re_sub(r\"#\\S+\", \"hashtag\")\n",
    "    # text = re_sub(r\"([!?.]){2,}\", r\" \\1 \")\n",
    "    # text = re_sub(r\"\\b(\\S*?)(.)\\2{2,}\\b\", r\"\\1\\2 \")\n",
    "    # text = re_sub(r\"([A-Z]){2,}\", \"allcaps\")\n",
    "    # text = re_sub(r'([\\w!.,?();*\\[\\]\":\\‚Äù\\‚Äú])([!.,?();*\\[\\]\":\\‚Äù\\‚Äú])', r'\\1 \\2')\n",
    "    # text = re_sub(r'([!.,?();*:\\[\\]\":\\‚Äù\\‚Äú])([\\w!.,?();*\\[\\]\":\\‚Äù\\‚Äú])', r'\\1 \\2')\n",
    "    # text = re_sub(r'(.)(<)', r'\\1 \\2')\n",
    "    # text = re_sub(r'(>)(.)', r'\\1 \\2')\n",
    "    # text = re_sub(r'[\\'\\`\\‚Äô\\‚Äò]', r'')\n",
    "    # text = re_sub(r'\\\\n', r' ')\n",
    "    text = re_sub(r\"https?:\\/\\/\\S+\\b|www\\.(\\w+\\.)+\\S*\", \" \")\n",
    "    # text = re_sub(r\"#(\\S+)\", r\" \") # replace #name with name\n",
    "    text = re_sub(r\"(:\\s?\\)|:-\\)|\\(\\s?:|\\(-:|:\\'\\))\", \" em_positive \")  # Smile -- :), : ), :-), (:, ( :, (-:, :')\n",
    "    text = re_sub(r\"(:\\s?D|:-D|x-?D|X-?D)\", \" em_positive \")  # Laugh -- :D, : D, :-D, xD, x-D, XD, X-D\n",
    "    text = re_sub(r\"(<3|:\\*)\", \" em_positive \")  # Love -- <3, :*\n",
    "    text = re_sub(r\"(;-?\\)|;-?D|\\(-?;)\", \" em_positive \")  # Wink -- ;-), ;), ;-D, ;D, (;,  (-;\n",
    "    text = re_sub(r'(:\\s?\\(|:-\\(|\\)\\s?:|\\)-:)', \" em_negative \")  # Sad -- :-(, : (, :(, ):, )-:\n",
    "    text = re_sub(r'(:,\\(|:\\'\\(|:\"\\()', \" em_negative \")  # Cry -- :,(, :'(, :\"(\n",
    "    text = re_sub(r\"(-|\\')\", \"\")  # remove &\n",
    "    text = re_sub(r\"/\", \" / \")\n",
    "    text = re_sub(r\"@[0-9]+-\", \" \")\n",
    "    text = re_sub(r\"@\\w+\", \" \")\n",
    "    text = re_sub(r\"{}{}[)dD]+|[)dD]+{}{}\".format(eyes, nose, nose, eyes), \" em_positive \")\n",
    "    text = re_sub(r\"{}{}p+\".format(eyes, nose), \" em_positive \")\n",
    "    text = re_sub(r\"{}{}\\(+|\\)+{}{}\".format(eyes, nose, nose, eyes), \" em_negative \")\n",
    "    text = re_sub(r\"{}{}[\\/|l*]\".format(eyes, nose), \" em_neutralface \")\n",
    "    text = re_sub(r\"<3\", \" heart \")\n",
    "    text = re_sub(r\"[-+]?[.\\d]*[\\d]+[:,.\\d]*\", \" \")\n",
    "    text = re_sub(r'-', r' ')\n",
    "    text = re_sub(r\"https?:\\/\\/\\S+\\b|www\\.(\\w+\\.)+\\S*\", \"  \")\n",
    "    #     text = re_sub(r\"([pls?s]){2,}\", r\"\\1\")\n",
    "    #     text = re_sub(r\"([plz?z]){2,}\", r\"\\1\")\n",
    "    text = re_sub(r'\\\\n', r' ')\n",
    "    text = re_sub(r\" app \", \" application \")\n",
    "    text = re_sub(r\"app\", \" application\")\n",
    "    text = re_sub(r\" wil \", \" will \")\n",
    "    text = re_sub(r\" im \", \" i am \")\n",
    "    text = re_sub(r\" al \", \" all \")\n",
    "    # text = re_sub(r\"<3\",\"love\")\n",
    "    text = re_sub(r\" sx \", \" sex \")\n",
    "    text = re_sub(r\" u \", \" you \")\n",
    "    text = re_sub(r\" r \", \" are \")\n",
    "    text = re_sub(r\" y \", \" why \")\n",
    "    text = re_sub(r\" Y \", \" WHY \")\n",
    "    text = re_sub(r\"Y \", \" WHY \")\n",
    "    text = re_sub(r\" hv \", \" have \")\n",
    "    text = re_sub(r\" c \", \" see \")\n",
    "    text = re_sub(r\" bcz \", \" because \")\n",
    "    text = re_sub(r\" coz \", \" because \")\n",
    "    text = re_sub(r\" v \", \" we \")\n",
    "    text = re_sub(r\" ppl \", \" people \")\n",
    "    text = re_sub(r\" pepl \", \" people \")\n",
    "    text = re_sub(r\" r b i \", \" rbi \")\n",
    "    text = re_sub(r\" R B I \", \" RBI \")\n",
    "    text = re_sub(r\" R b i \", \" rbi \")\n",
    "    text = re_sub(r\" R \", \" ARE \")\n",
    "    text = re_sub(r\" hav \", \" have \")\n",
    "    text = re_sub(r\"R \", \" ARE \")\n",
    "    text = re_sub(r\" U \", \" you \")\n",
    "    text = re_sub(r\" üëé \", \" em_negative \")\n",
    "    text = re_sub(r\"U \", \" you \")\n",
    "    text = re_sub(r\" pls \", \" please \")\n",
    "    text = re_sub(r\"Pls \", \"Please \")\n",
    "    text = re_sub(r\"plz \", \"please \")\n",
    "    text = re_sub(r\"Plz \", \"Please \")\n",
    "    text = re_sub(r\"PLZ \", \"Please \")\n",
    "    text = re_sub(r\"Pls\", \"Please \")\n",
    "    text = re_sub(r\"plz\", \"please \")\n",
    "    text = re_sub(r\"Plz\", \"Please \")\n",
    "    text = re_sub(r\"PLZ\", \"Please \")\n",
    "    text = re_sub(r\" thankz \", \" thanks \")\n",
    "    text = re_sub(r\" thnx \", \" thanks \")\n",
    "    text = re_sub(r\"fuck\\w+ \", \" fuck \")\n",
    "    text = re_sub(r\"f\\*\\* \", \" fuck \")\n",
    "    text = re_sub(r\"\\*\\*\\*k \", \" fuck \")\n",
    "    text = re_sub(r\"F\\*\\* \", \" fuck \")\n",
    "    text = re_sub(r\"mo\\*\\*\\*\\*\\* \", \" fucker \")\n",
    "    text = re_sub(r\"b\\*\\*\\*\\* \", \" blody \")\n",
    "    text = re_sub(r\" mc \", \" fucker \")\n",
    "    text = re_sub(r\" MC \", \" fucker \")\n",
    "    text = re_sub(r\" wtf \", \" fuck \")\n",
    "    text = re_sub(r\" ch\\*\\*\\*ya \", \" fucker \")\n",
    "    text = re_sub(r\" ch\\*\\*Tya \", \" fucker \")\n",
    "    text = re_sub(r\" ch\\*\\*Tia \", \" fucker \")\n",
    "    text = re_sub(r\" C\\*\\*\\*yas \", \" fucker \")\n",
    "    text = re_sub(r\"l\\*\\*\\*\\* \", \"shit \")\n",
    "    text = re_sub(r\" A\\*\\*\\*\\*\\*\\*S\", \" ASSHOLES\")\n",
    "    text = re_sub(r\" di\\*\\*\\*\\*s\", \" cker\")\n",
    "    text = re_sub(r\" nd \", \" and \")\n",
    "    text = re_sub(r\"Nd \", \"and \")\n",
    "    text = re_sub(r\"([!?!]){2,}\", r\"! \")\n",
    "    text = re_sub(r\"([.?.]){2,}\", r\". \")\n",
    "    text = re_sub(r\"([*?*]){2,}\", r\"* \")\n",
    "    text = re_sub(r\"([,?,]){2,}\", r\", \")\n",
    "    text = re_sub(r\"ha?ha\", r\" em_positive \")\n",
    "    # text = re_sub(r\"([!]){2,}\", r\"! \")\n",
    "    # text = re_sub(r\"([.]){2,}\", r\". \")\n",
    "    # text = re_sub(r\"([*]){2,}\", r\"* \")\n",
    "    # text = re_sub(r\"([,]){2,}\", r\", \")\n",
    "    # text = re_sub(r\"\\n\\r\", \" \")\n",
    "    text = re_sub(r\"(ind[vs]pak)\", \" india versus pakistan \")\n",
    "    text = re_sub(r\"(pak[vs]ind)\", \" pakistan versus india \")\n",
    "    text = re_sub(r\"(indvsuae)\", \" india versus United Arab Emirates \")\n",
    "    text = re_sub(r\"[sS]hut[Dd]own[jnuJNU]\", \" shut down jnu \")\n",
    "    # text = re_sub(r\"ShutDownJNU\", \" shut down jnu \")\n",
    "    # text = re_sub(r\"[-+]?[.\\d]*[\\d]+[:,.\\d]*\", \" number \")\n",
    "    # text = re_sub(r\"(.)\\1\\1+\", r\"\\1\") # remove funnnnny --> funny\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4abab790",
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAGS = re.MULTILINE | re.DOTALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dfadd2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df = pd.read_csv(twit_path)\n",
    "my_df['gender'] = my_df['gender'].map({'female\\r\\n': 0, 'male\\r\\n': 1})\n",
    "my_df['text'] = my_df['text'].apply(lambda x: \" \".join(stem(x) for x in x.split()))\n",
    "stop = stopwords.words('english')\n",
    "my_df['text'] = my_df['text'].apply(lambda x: \" \".join(x for x in x.split(\" \") if x not in stop))\n",
    "my_word_stop = ['the', 'in', 'of', 'is', 'a', 'to', 'an', 'be']\n",
    "my_df['text'] = my_df['text'].apply(lambda x: \" \".join(x for x in x.split(\" \") if x not in my_word_stop))\n",
    "my_df['text'] = my_df['text'].apply(lambda x: \" \".join(tokenize(x) for x in x.split(\" \")))\n",
    "# my_df['text'] = my_df['text'].apply(lambda x: \" \".join(x for x in text_processor.pre_process_doc(x) if x not in stop))\n",
    "freq = pd.Series(' '.join(my_df['text']).split()).value_counts()[:50]\n",
    "freq = list(freq.index)\n",
    "my_df['text'] = my_df['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e448112f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "train_x = my_df.text\n",
    "train_y = my_df.gender\n",
    "\n",
    "# create a new Tokenizer\n",
    "max_num = 2000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_num)\n",
    "# feed our tweets to the Tokenizer\n",
    "tokenizer.fit_on_texts(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c4fc2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "dic_path = \"C:\\\\Users\\\\morzm\\\\OneDrive\\\\Asztali g√©p\\\\txts\\\\model\\\\en_dictionary.json\"\n",
    "\n",
    "# Tokenizers come with a convenient list of words and IDs\n",
    "dictionary = tokenizer.word_index\n",
    "# Let's save this out so we can use it later\n",
    "with open(dic_path, 'w+') as dictionary_file:\n",
    "    json.dump(dictionary, dictionary_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3fab0658",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_text_to_index_array(text):\n",
    "    # one really important thing that `text_to_word_sequence` does\n",
    "    # is make all texts the same length -- in this case, the length\n",
    "    # of the longest text in the set.\n",
    "    temp_wordIndices = []\n",
    "    for word in kpt.text_to_word_sequence(text):\n",
    "        if word in dictionary:\n",
    "            temp_wordIndices.append(dictionary[word])\n",
    "    return temp_wordIndices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f25df4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "allWordIndices = []\n",
    "# for each tweet, change each token to its ID in the Tokenizer's word_index\n",
    "for text in train_x:\n",
    "    wordIndices = convert_text_to_index_array(text)\n",
    "    allWordIndices.append(wordIndices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b0598ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we have a list of all tweets converted to index arrays.\n",
    "# cast as an array for future usage.\n",
    "allWordIndices = np.asarray(allWordIndices, dtype=\"object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aa8d5297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 1024)              2049024   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 512)               524800    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2)                 1026      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,574,850\n",
      "Trainable params: 2,574,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# create one-hot matrices out of the indexed tweets\n",
    "train_x = tokenizer.sequences_to_matrix(allWordIndices, mode='binary')\n",
    "# treat the labels as categories\n",
    "train_y = np_utils.to_categorical(train_y, 2)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(1024, activation='relu', input_shape=(max_num,)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(512, activation='sigmoid'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b01bbe32",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelDir = \"C:\\\\Users\\\\morzm\\\\OneDrive\\\\Asztali g√©p\\\\txts\\\\model\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7a741181",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=\"sequencing_the_data_try_n_error.{epoch:02d}-{val_loss:.4f}-{val_acc:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "csv_logger = CSVLogger(os.path.join(modelDir,'en_model_log.csv'), append=True, separator=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "683cb622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "8438/8438 [==============================] - 81s 9ms/step - loss: 0.6702 - accuracy: 0.5865 - val_loss: 0.6654 - val_accuracy: 0.5887\n",
      "Epoch 2/2\n",
      "8438/8438 [==============================] - 77s 9ms/step - loss: 0.6484 - accuracy: 0.6093 - val_loss: 0.6707 - val_accuracy: 0.5850\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2235c8ac460>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_x, train_y,\n",
    "    batch_size=32,\n",
    "    epochs=2,\n",
    "    verbose=1,\n",
    "    validation_split=0.1,\n",
    "    shuffle=True,callbacks = [csv_logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "738f21bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                     text  gender\n",
      "0                              One watch ‚Ä¶ Available Feb.       1\n",
      "1       There complete sets box batman minifigs. Find ...       1\n",
      "2                        If (maybe ), come arrangement. üëç       1\n",
      "3       heard numerous toyworlds numbering, despite wa...       1\n",
      "4            Just heard Web Dev meetup Hutt. Spread word!       1\n",
      "...                                                   ...     ...\n",
      "299995  Hey! Is studio? Heres q NZH. Who said Yes! Pri...       0\n",
      "299996  God lett asswipe post selfpity piece Mothers D...       0\n",
      "299997  Next week #HottieRatings pinch #SilverFox stal...       0\n",
      "299998  Story relevant cos timeline. say though. We co...       0\n",
      "299999          #SilverFox rat system GO FOR LAUNCH! #OTD       0\n",
      "\n",
      "[300000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(my_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "32af479b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved model!\n"
     ]
    }
   ],
   "source": [
    "model_json = model.to_json()\n",
    "\n",
    "with open(model_path, 'w+') as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "model.save_weights(os.path.join(modelDir, 'en_model.h5'))\n",
    "\n",
    "print('saved model!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2d86316f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in your saved model structure\n",
    "json_file = open(model_path, 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "# and create a model from that\n",
    "model = model_from_json(loaded_model_json)\n",
    "# and weight your nodes with your saved values\n",
    "model.load_weights(\"C:\\\\Users\\\\morzm\\\\jup_txts\\\\csv\\\\model\\\\en_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "99a302a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in our saved dictionary\n",
    "with open(dic_path, 'r') as dictionary_file:\n",
    "    dictionary = json.load(dictionary_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a499ca1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_text_to_index_array(text):\n",
    "    words = kpt.text_to_word_sequence(text)\n",
    "    wordIndices = []\n",
    "    no_word = 0\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            wordIndices.append(dictionary[word])\n",
    "        else:\n",
    "            # print(\"'%s' not in training corpus; ignoring.\" %(word))\n",
    "            not_found_word_list.append(word)\n",
    "            no_word = no_word + 1\n",
    "    return wordIndices, no_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "598e01d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_xml_path = \"C:\\\\Users\\\\morzm\\\\jup_txts\\\\csv\\\\test_xml\\\\text\\\\\"\n",
    "result_xml_path = \"C:\\\\Users\\\\morzm\\\\jup_txts\\\\csv\\\\test_xml\\\\output\\\\\"\n",
    "model_path = \"C:\\\\Users\\\\morzm\\\\jup_txts\\csv\\\\model\\\\en_model.json\"\n",
    "dic_path = \"C:\\\\Users\\\\morzm\\\\jup_txts\\\\csv\\\\model\\\\en_dictionary.json\"\n",
    "temp_twit = os.path.join(outputDir, 'temp_twit.csv')\n",
    "model_weight = os.path.join(modelDir, 'en_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2aa2ec0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----reading xml file data\n",
    "for filename in os.listdir(test_xml_path):\n",
    "    if not filename.endswith('.xml'): continue\n",
    "    # ----reading file name\n",
    "    author_id = os.path.splitext(filename)[0]\n",
    "    # ----reading xml full path\n",
    "    xml_fullname = os.path.join(test_xml_path, filename)\n",
    "    tree = etree.parse(xml_fullname)\n",
    "    root = tree.getroot()\n",
    "    # ----reading xml file data and storing in temp csv\n",
    "    with open(temp_twit, 'w+') as csvfile:\n",
    "        fieldnames = ['text']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for i in range(len(root[0])):\n",
    "            writer.writerow({'text': (root[0][i].text).encode('utf-8')})\n",
    "    csvfile.close()\n",
    "    # --reading temp csv file\n",
    "    my_df = pd.read_csv(temp_twit)\n",
    "    my_df['text'] = my_df['text'].apply(lambda x: \" \".join(stem(x) for x in x.split()))\n",
    "    stop = stopwords.words('english')\n",
    "    my_df['text'] = my_df['text'].apply(lambda x: \" \".join(x for x in x.split(\" \") if x not in stop))\n",
    "    my_df['text'] = my_df['text'].apply(lambda x: \" \".join(tokenize(x) for x in x.split(\" \")))\n",
    "    # my_df['text'] = my_df['text'].apply(lambda x: \" \".join(x for x in text_processor.pre_process_doc(x) if x not in stop))\n",
    "    # we're still going to use a Tokenizer here, but we don't need to fit it\n",
    "    tokenizer = Tokenizer(num_words=max_num)\n",
    "    # for human-friendly printing\n",
    "    labels = ['female', 'male']\n",
    "    # this utility makes sure that all the words in your input\n",
    "    # are registered in the dictionary\n",
    "    # before trying to turn them into a matrix.\n",
    "    not_found_word_list = []\n",
    "    male = 0\n",
    "    female = 0\n",
    "    for row in my_df.text:\n",
    "        # okay here's the interactive part\n",
    "        evalSentence = row\n",
    "        # format your input for the neural net\n",
    "        testArr, no_word = convert_text_to_index_array(evalSentence)\n",
    "        input = tokenizer.sequences_to_matrix([testArr], mode='binary')\n",
    "        # predict which bucket your input belongs in\n",
    "        pred = model.predict(input)\n",
    "        gender = labels[np.argmax(pred)]\n",
    "        if gender == \"male\":\n",
    "            male += 1\n",
    "        else:\n",
    "            female += 1\n",
    "    if male >= female:\n",
    "        gender = \"male\"\n",
    "    else:\n",
    "        gender = \"female\"\n",
    "    text_to_write = \"\"\"<author id='%s'\\n\\tlang='en'\\n\\tgender_txt='%s'\\n/>\"\"\" % (author_id, gender)\n",
    "    xml_result = os.path.join(result_xml_path, filename)\n",
    "    fo = open(xml_result, \"w+\")\n",
    "    fo.write(text_to_write)\n",
    "    fo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "90abd21d",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [49]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m sample_1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mI hate that the dismal weather had me down for so long, \u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124mwhen will it break! Ugh, when does happiness return? The sun is \u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124mblinding and the puffy clouds are too thin. I can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt wait for the weekend.\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# The first value is a \"fake\" class (this is the expected input)\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_1\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py:902\u001b[0m, in \u001b[0;36mTensorShape.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    901\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_v2_behavior:\n\u001b[1;32m--> 902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dims\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mvalue\n\u001b[0;32m    903\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    904\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dims[key]\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Predicting a new instance\n",
    "\n",
    "# Notice we have both positive and negative words here\n",
    "sample_1 = \"\"\"I hate that the dismal weather had me down for so long, \n",
    "when will it break! Ugh, when does happiness return? The sun is \n",
    "blinding and the puffy clouds are too thin. I can't wait for the weekend.\"\"\"\n",
    "\n",
    "# The first value is a \"fake\" class (this is the expected input)\n",
    "\n",
    "model.predict(sample_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0848712",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
